{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"cleveland.csv\")\n",
    "# Preprocess the data\n",
    "data[\"num\"] = data[\"num\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "# Change num column name to make more sense\n",
    "data = data.rename({'num':'disease'}, axis=1)\n",
    "\n",
    "data.replace('?', pd.NA, inplace=True)\n",
    "# TODO: may want replace ? with the mode of the column\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n",
    "for col in non_numeric_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define K-Nearest Neighbor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKNN:\n",
    "    def __init__(self, k=5, dist_metric='euclidean'):\n",
    "        self.k = k\n",
    "        self.dist_metric = dist_metric\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def compute_distance(self, x1, x2):\n",
    "        if self.dist_metric == 'euclidean':\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.dist_metric == 'manhattan':\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        y_pred = []\n",
    "        for x in X_test:\n",
    "            distances = [self.compute_distance(x, x_train) for x_train in self.X_train]\n",
    "            sorted_indices = sorted(range(len(distances)), key=lambda i: distances[i])\n",
    "            k_indices = sorted_indices[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            y_pred_fold = self.most_common(k_nearest_labels)\n",
    "            # Append the predicted label to y_pred\n",
    "            y_pred.append(y_pred_fold)\n",
    "        # Convert y_pred to numpy array for compatibility with evaluation functions\n",
    "        y_pred = np.array(y_pred)\n",
    "        return y_pred\n",
    "    \n",
    "    def most_common(self, lst):\n",
    "        return max(set(lst), key=lst.count)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred, precision_scores, recall_scores, f1_scores = self.predict(X_test)\n",
    "        # Print precision, recall, and F1 scores for each fold\n",
    "        for i in range(len(precision_scores)):\n",
    "            print(f\"Fold {i+1}: Precision = {precision_scores[i]}, Recall = {recall_scores[i]}, F1 Score = {f1_scores[i]}\")\n",
    "        # Compute mean precision, recall, and F1 scores\n",
    "        mean_precision = np.mean(precision_scores)\n",
    "        mean_recall = np.mean(recall_scores)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        # Print mean precision, recall, and F1 scores\n",
    "        print(f\"Mean Precision: {mean_precision}, Mean Recall: {mean_recall}, Mean F1 Score: {mean_f1}\")\n",
    "        return mean_precision, mean_recall, mean_f1\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'k': self.k, 'dist_metric': self.dist_metric}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        if 'k' in params:\n",
    "            self.k = params['k']\n",
    "        if 'dist_metric' in params:\n",
    "            self.dist_metric = params['dist_metric']\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Number and Combination Of Features to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Features\n",
    "features = [\"age\",\"sex\",\"cp\",\"trestbps\",\"chol\",\n",
    "            \"fbs\",\"restecg\",\"thalach\",\"exang\",\n",
    "            \"oldpeak\",\"slope\",\"ca\",\"thal\"] # all features\n",
    "# Split features and target variable\n",
    "X = data[features].values\n",
    "y = data[\"disease\"].values\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Define the estimator (model) to be used\n",
    "estimator = RandomForestClassifier(n_estimators=100)\n",
    "# Define the range of number of features to test\n",
    "num_features_range = range(1, len(features) + 1)  # Test from 1 to the total number of features\n",
    "# Initialize lists to store results\n",
    "best_feature_combination = []\n",
    "best_f1_score = 0\n",
    "feature_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Loop over different numbers of features\n",
    "for num_features_to_select in num_features_range:\n",
    "    # Initialize RFE with the chosen estimator and number of features to select\n",
    "    rfe = RFE(estimator, n_features_to_select=num_features_to_select)\n",
    "    # Fit RFE to the training data\n",
    "    rfe.fit(X_train, y_train)\n",
    "    # Get the selected features\n",
    "    selected_features = np.array(features)[rfe.support_]\n",
    "    # Get the transformed training and testing data with selected features\n",
    "    X_train_selected = rfe.transform(X_train)\n",
    "    X_test_selected = rfe.transform(X_test)\n",
    "    # Train the model using selected features\n",
    "    estimator.fit(X_train_selected, y_train)\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = estimator.predict(X_test_selected)\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    # Append results to lists\n",
    "    feature_scores.append(num_features_to_select)\n",
    "    f1_scores.append(f1)\n",
    "    # Update best feature combination if current F1 score is better\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_feature_combination = selected_features\n",
    "\n",
    "# Find the index of the maximum F1 score\n",
    "best_f1_index = np.argmax(f1_scores)\n",
    "best_num_features = feature_scores[best_f1_index]\n",
    "best_f1_score = f1_scores[best_f1_index]\n",
    "\n",
    "print(f\"Best F1 Score: {best_f1_score} with {best_num_features} features selected.\")\n",
    "print(\"Best Feature Combination:\", best_feature_combination)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(feature_scores, f1_scores, marker='o')\n",
    "plt.xlabel('Number of Features Selected')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs. Number of Features Selected')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Up On the Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[best_feature_combination].values\n",
    "y = data[\"disease\"].values\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Amount of K-Neighbors to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krange = range(1,21)\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'k': krange,\n",
    "    'dist_metric': ['euclidean', 'manhattan'],\n",
    "}\n",
    "\n",
    "# Create an instance of MyKNN\n",
    "knn = MyKNN()\n",
    "# Create GridSearchCV object\n",
    "cv_num = 10\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=cv_num, scoring='accuracy')\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Extract results from the grid search\n",
    "results = grid_search.cv_results_\n",
    "mean_test_scores = results['mean_test_score']\n",
    "params = results['params']\n",
    "data_results = pd.DataFrame({\n",
    "    'k_values': [p['k'] for p in params],  # Extract k values\n",
    "    'dist_metric': [p['dist_metric'] for p in params],  # Extract distance metrics\n",
    "    'mean_accuracy': mean_test_scores  # Mean accuracy scores\n",
    "})\n",
    "\n",
    "# Plot Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=data_results, x='k_values', y='mean_accuracy', hue='dist_metric', marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('Performance of k-NN with different distance metrics')\n",
    "plt.grid(True)\n",
    "plt.xticks(krange)\n",
    "plt.legend(title='Distance Metric')\n",
    "plt.show()\n",
    "\n",
    "# Perform cross-validation with the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_k = best_params['k']\n",
    "best_dist_metric = best_params['dist_metric']\n",
    "print(f\"Best parameters: {best_params}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Best Parameters and Preform 10-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn_best = MyKNN(k=best_k, dist_metric=best_dist_metric)\n",
    "cv_scores = cross_val_score(knn_best, X_train, y_train, cv=cv_num, scoring='accuracy')\n",
    "cv = KFold(n_splits=cv_num, shuffle=True)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "# Print precision, recall, and F1 scores for each fold during cross-validation\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X_train)):\n",
    "    X_train_fold, X_test_fold = X_train[train_idx], X_train[test_idx]\n",
    "    y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]\n",
    "    knn_best.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = knn_best.predict(X_test_fold)\n",
    "    precision_fold = precision_score(y_test_fold, y_pred_fold)\n",
    "    recall_fold = recall_score(y_test_fold, y_pred_fold)\n",
    "    f1_fold = f1_score(y_test_fold, y_pred_fold)\n",
    "    print(f\"Fold {fold_idx + 1}: Precision = {precision_fold}, Recall = {recall_fold}, F1 Score = {f1_fold}\")# Append the scores to the respective lists\n",
    "    precision_scores.append(precision_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "    f1_scores.append(f1_fold)\n",
    "\n",
    "# Calculate the mean precision, recall, and F1 score\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print the mean precision, recall, and F1 score\n",
    "print(f\"\\nMean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"forestfires.csv\")\n",
    "data[\"area\"] = data[\"area\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n",
    "for col in non_numeric_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Features\n",
    "features = [\"X\",\"Y\",\"FFMC\",\"DMC\",\"DC\",\n",
    "            \"ISI\",\"temp\",\"RH\",\"wind\",\"rain\"] # all features\n",
    "# Split features and target variable\n",
    "X = data[features].values\n",
    "y = data[\"area\"].values\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Define the estimator (model) to be used\n",
    "estimator = RandomForestClassifier(n_estimators=100)\n",
    "# Define the range of number of features to test\n",
    "num_features_range = range(1, len(features) + 1)  # Test from 1 to the total number of features\n",
    "# Initialize lists to store results\n",
    "best_feature_combination = []\n",
    "best_f1_score = 0\n",
    "feature_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Loop over different numbers of features\n",
    "for num_features_to_select in num_features_range:\n",
    "    # Initialize RFE with the chosen estimator and number of features to select\n",
    "    rfe = RFE(estimator, n_features_to_select=num_features_to_select)\n",
    "    # Fit RFE to the training data\n",
    "    rfe.fit(X_train, y_train)\n",
    "    # Get the selected features\n",
    "    selected_features = np.array(features)[rfe.support_]\n",
    "    # Get the transformed training and testing data with selected features\n",
    "    X_train_selected = rfe.transform(X_train)\n",
    "    X_test_selected = rfe.transform(X_test)\n",
    "    # Train the model using selected features\n",
    "    estimator.fit(X_train_selected, y_train)\n",
    "    # Make predictions on the testing data\n",
    "    y_pred = estimator.predict(X_test_selected)\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    # Append results to lists\n",
    "    feature_scores.append(num_features_to_select)\n",
    "    f1_scores.append(f1)\n",
    "    # Update best feature combination if current F1 score is better\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_feature_combination = selected_features\n",
    "\n",
    "# Find the index of the maximum F1 score\n",
    "best_f1_index = np.argmax(f1_scores)\n",
    "best_num_features = feature_scores[best_f1_index]\n",
    "best_f1_score = f1_scores[best_f1_index]\n",
    "\n",
    "print(f\"Best F1 Score: {best_f1_score} with {best_num_features} features selected.\")\n",
    "print(\"Best Feature Combination:\", best_feature_combination)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(feature_scores, f1_scores, marker='o')\n",
    "plt.xlabel('Number of Features Selected')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs. Number of Features Selected')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[best_feature_combination].values\n",
    "y = data[\"area\"].values\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krange = range(1,11)\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'k': krange,\n",
    "    'dist_metric': ['euclidean', 'manhattan'],\n",
    "}\n",
    "\n",
    "# Create an instance of MyKNN\n",
    "knn = MyKNN()\n",
    "# Create GridSearchCV object\n",
    "cv_num = 10\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=cv_num, scoring='accuracy')\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Extract results from the grid search\n",
    "results = grid_search.cv_results_\n",
    "mean_test_scores = results['mean_test_score']\n",
    "params = results['params']\n",
    "data_results = pd.DataFrame({\n",
    "    'k_values': [p['k'] for p in params],  # Extract k values\n",
    "    'dist_metric': [p['dist_metric'] for p in params],  # Extract distance metrics\n",
    "    'mean_accuracy': mean_test_scores  # Mean accuracy scores\n",
    "})\n",
    "\n",
    "# Plot Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=data_results, x='k_values', y='mean_accuracy', hue='dist_metric', marker='o')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.title('Performance of k-NN with different distance metrics')\n",
    "plt.grid(True)\n",
    "plt.xticks(krange)\n",
    "plt.legend(title='Distance Metric')\n",
    "plt.show()\n",
    "\n",
    "# Perform cross-validation with the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_k = best_params['k']\n",
    "best_dist_metric = best_params['dist_metric']\n",
    "print(f\"Best parameters: {best_params}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best = MyKNN(k=best_k, dist_metric=best_dist_metric)\n",
    "cv_scores = cross_val_score(knn_best, X_train, y_train, cv=cv_num, scoring='accuracy')\n",
    "cv = KFold(n_splits=cv_num, shuffle=True)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "# Print precision, recall, and F1 scores for each fold during cross-validation\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X_train)):\n",
    "    X_train_fold, X_test_fold = X_train[train_idx], X_train[test_idx]\n",
    "    y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]\n",
    "    knn_best.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = knn_best.predict(X_test_fold)\n",
    "    precision_fold = precision_score(y_test_fold, y_pred_fold)\n",
    "    recall_fold = recall_score(y_test_fold, y_pred_fold)\n",
    "    f1_fold = f1_score(y_test_fold, y_pred_fold)\n",
    "    print(f\"Fold {fold_idx + 1}: Precision = {precision_fold}, Recall = {recall_fold}, F1 Score = {f1_fold}\")# Append the scores to the respective lists\n",
    "    precision_scores.append(precision_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "    f1_scores.append(f1_fold)\n",
    "\n",
    "# Calculate the mean precision, recall, and F1 score\n",
    "mean_precision = np.mean(precision_scores)\n",
    "mean_recall = np.mean(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print the mean precision, recall, and F1 score\n",
    "print(f\"\\nMean Precision: {mean_precision}\")\n",
    "print(f\"Mean Recall: {mean_recall}\")\n",
    "print(f\"Mean F1 Score: {mean_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
